 # Config for training ALAE on evox at resolution 256x256

NAME: evox
DATASET:
  PART_COUNT: 1 # how many times to split up the dataset (for parallelizing?)
  SIZE: 11664
  PATH: /home/beum/scratch/ALAE/data/datasets/evox_64x64_1/evox_64x64_1-r%02d.tfrecords.%03d
  PATH_TEST: /home/beum/scratch/ALAE/data/datasets/evox-test_64x64_1/evox-test_64x64_1-r%02d.tfrecords.%03d
  MAX_RESOLUTION_LEVEL: 7

  SAMPLES_PATH: no_path #dataset_samples/cars
  STYLE_MIX_PATH: style_mixing/test_images/set_cars
MODEL:
  LATENT_SPACE_SIZE: 512
  LAYER_COUNT: 7
  MAX_CHANNEL_COUNT: 512
  START_CHANNEL_COUNT: 32
  DLATENT_AVG_BETA: 0.995
  MAPPING_LAYERS: 8
  CHANNELS: 1 #greyscale
OUTPUT_DIR: training_artifacts/cars
TRAIN:
  BASE_LEARNING_RATE: 0.002
  EPOCHS_PER_LOD: 2
  LEARNING_DECAY_RATE: 0.1
  LEARNING_DECAY_STEPS: []
  TRAIN_EPOCHS: 112
  #                    4    8   16    32    64    128
  LOD_2_BATCH_8GPU: [512, 256, 128, 64, 32, 32, 32, 32, 32]
  LOD_2_BATCH_4GPU: [512, 256, 128, 64, 32, 32, 32, 32, 16]
  LOD_2_BATCH_2GPU: [512, 256, 128, 64, 32, 32, 16]
  LOD_2_BATCH_1GPU: [512, 256, 128, 64, 32, 16]

  LEARNING_RATES: [.0015, .0015, .0015, .0015, .0015, .0015, .002, .003, .003]
