2025-03-22 11:47:26,020 logger INFO: Namespace(config_file='evox', opts=[])
2025-03-22 11:47:26,021 logger INFO: World size: 1
2025-03-22 11:47:26,022 logger INFO: Loaded configuration file configs/evox.yaml
2025-03-22 11:47:26,022 logger INFO: 
 # Config for training ALAE on evox at resolution 256x256

NAME: evox
DATASET:
  PART_COUNT: 1 # how many times to split up the dataset (for parallelizing?)
  SIZE: 11647
  PATH: /home/beum/projects/def-webbr/beum/ALAE/data/datasets/evox/evox-r%02d.tfrecords.%03d
  PATH_TEST: /home/beum/projects/def-webbr/beum/ALAE/data/datasets/evox-test/evox-test-r%02d.tfrecords.%03d
  MAX_RESOLUTION_LEVEL: 7

  SAMPLES_PATH: no_path #dataset_samples/cars
  STYLE_MIX_PATH: style_mixing/test_images/set_cars
MODEL:
  LATENT_SPACE_SIZE: 512
  LAYER_COUNT: 7
  MAX_CHANNEL_COUNT: 512
  START_CHANNEL_COUNT: 32
  DLATENT_AVG_BETA: 0.995
  MAPPING_LAYERS: 8
  CHANNELS: 1 #greyscale
OUTPUT_DIR: training_artifacts/cars
TRAIN:
  BASE_LEARNING_RATE: 0.002
  EPOCHS_PER_LOD: 2
  LEARNING_DECAY_RATE: 0.1
  LEARNING_DECAY_STEPS: []
  TRAIN_EPOCHS: 112
  #                    4    8   16    32    64    128
  LOD_2_BATCH_8GPU: [512, 256, 128,   64,   32,    32,    32,       32,        32]
  LOD_2_BATCH_4GPU: [512, 256, 128,   64,   32,    32,    32,       32,        16]
  LOD_2_BATCH_2GPU: [512, 256, 128,   64,   32,    32,    16]
  LOD_2_BATCH_1GPU: [512, 256, 128,   64,   32,    16]

  LEARNING_RATES: [0.0015,  0.0015,   0.0015,   0.0015,  0.0015,   0.0015,     0.002,     0.003,    0.003]

2025-03-22 11:47:26,022 logger INFO: Running with config:
DATASET:
  FFHQ_SOURCE: /data/datasets/ffhq-dataset/tfrecords/ffhq/ffhq-r%02d.tfrecords
  FLIP_IMAGES: True
  MAX_RESOLUTION_LEVEL: 7
  PART_COUNT: 1
  PART_COUNT_TEST: 1
  PATH: /home/beum/projects/def-webbr/beum/ALAE/data/datasets/evox/evox-r%02d.tfrecords.%03d
  PATH_TEST: /home/beum/projects/def-webbr/beum/ALAE/data/datasets/evox-test/evox-test-r%02d.tfrecords.%03d
  SAMPLES_PATH: no_path
  SIZE: 11647
  SIZE_TEST: 10000
  STYLE_MIX_PATH: style_mixing/test_images/set_cars
MODEL:
  CHANNELS: 1
  DLATENT_AVG_BETA: 0.995
  ENCODER: EncoderDefault
  GENERATOR: GeneratorDefault
  LATENT_SPACE_SIZE: 512
  LAYER_COUNT: 7
  MAPPING_D: MappingD
  MAPPING_F: MappingF
  MAPPING_LAYERS: 8
  MAX_CHANNEL_COUNT: 512
  START_CHANNEL_COUNT: 32
  STYLE_MIXING_PROB: 0.9
  TRUNCATIOM_CUTOFF: 8
  TRUNCATIOM_PSI: 0.7
  Z_REGRESSION: False
NAME: evox
OUTPUT_DIR: training_artifacts/cars
PPL_CELEBA_ADJUSTMENT: False
TRAIN:
  ADAM_BETA_0: 0.0
  ADAM_BETA_1: 0.99
  BASE_LEARNING_RATE: 0.002
  EPOCHS_PER_LOD: 2
  LEARNING_DECAY_RATE: 0.1
  LEARNING_DECAY_STEPS: []
  LEARNING_RATES: [0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.002, 0.003, 0.003]
  LOD_2_BATCH_1GPU: [512, 256, 128, 64, 32, 16]
  LOD_2_BATCH_2GPU: [512, 256, 128, 64, 32, 32, 16]
  LOD_2_BATCH_4GPU: [512, 256, 128, 64, 32, 32, 32, 32, 16]
  LOD_2_BATCH_8GPU: [512, 256, 128, 64, 32, 32, 32, 32, 32]
  REPORT_FREQ: [100, 80, 60, 30, 20, 10, 10, 5, 5]
  SNAPSHOT_FREQ: [300, 300, 300, 100, 50, 30, 20, 20, 10]
  TRAIN_EPOCHS: 112
2025-03-22 11:47:44,802 logger INFO: Trainable parameters generator:
2025-03-22 11:47:44,803 logger INFO: Trainable parameters discriminator:
2025-03-22 11:47:44,839 logger INFO: No checkpoint found. Initializing model from scratch
2025-03-22 11:47:44,840 logger INFO: Starting from epoch: 0
2025-03-22 11:47:45,597 logger INFO: ################################################################################
2025-03-22 11:47:45,597 logger INFO: # Switching LOD to 0
2025-03-22 11:47:45,597 logger INFO: # Starting transition
2025-03-22 11:47:45,597 logger INFO: ################################################################################
2025-03-22 11:47:45,598 logger INFO: ################################################################################
2025-03-22 11:47:45,598 logger INFO: # Transition ended
2025-03-22 11:47:45,598 logger INFO: ################################################################################
2025-03-22 11:47:45,599 logger INFO: Batch size: 512, Batch size per GPU: 512, LOD: 0 - 4x4, blend: 1.000, dataset size: 11647
2025-03-22 11:47:53,508 logger INFO: Saving checkpoint to training_artifacts/cars/model_tmp_lod0.pth
2025-03-22 11:47:53,575 logger INFO: 
[1/112] - ptime: 7.91, loss_d: 5.9890652, loss_g: 0.9864029, lae: 1.6407334, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 1018.734375",
2025-03-22 11:47:53,590 logger INFO: Batch size: 512, Batch size per GPU: 512, LOD: 0 - 4x4, blend: 1.000, dataset size: 11647
2025-03-22 11:48:01,164 logger INFO: Saving checkpoint to training_artifacts/cars/model_tmp_lod0.pth
2025-03-22 11:48:01,165 logger INFO: 
[2/112] - ptime: 7.57, loss_d: 5.4344234, loss_g: 0.8574994, lae: 1.5558448, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 1018.753906",
2025-03-22 11:48:01,194 logger INFO: ################################################################################
2025-03-22 11:48:01,194 logger INFO: # Switching LOD to 1
2025-03-22 11:48:01,194 logger INFO: # Starting transition
2025-03-22 11:48:01,195 logger INFO: ################################################################################
2025-03-22 11:48:01,195 logger INFO: Batch size: 256, Batch size per GPU: 256, LOD: 1 - 8x8, blend: 0.000, dataset size: 11647
2025-03-22 11:48:28,367 logger INFO: Saving checkpoint to training_artifacts/cars/model_tmp_lod1.pth
2025-03-22 11:48:28,368 logger INFO: 
[3/112] - ptime: 27.17, loss_d: 7.5647550, loss_g: 2.2595205, lae: 1.5265635, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 2132.128418",
2025-03-22 11:48:28,449 logger INFO: ################################################################################
2025-03-22 11:48:28,449 logger INFO: # Transition ended
2025-03-22 11:48:28,450 logger INFO: ################################################################################
2025-03-22 11:48:28,450 logger INFO: Batch size: 256, Batch size per GPU: 256, LOD: 1 - 8x8, blend: 1.000, dataset size: 11647
2025-03-22 11:48:55,296 logger INFO: Saving checkpoint to training_artifacts/cars/model_tmp_lod1.pth
2025-03-22 11:48:55,297 logger INFO: 
[4/112] - ptime: 26.84, loss_d: 5.4427609, loss_g: 2.4309108, lae: 1.0254489, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 2132.128418",
2025-03-22 11:48:55,334 logger INFO: ################################################################################
2025-03-22 11:48:55,335 logger INFO: # Switching LOD to 2
2025-03-22 11:48:55,335 logger INFO: # Starting transition
2025-03-22 11:48:55,335 logger INFO: ################################################################################
2025-03-22 11:48:55,335 logger INFO: Batch size: 128, Batch size per GPU: 128, LOD: 2 - 16x16, blend: 0.000, dataset size: 11647
2025-03-22 11:50:26,172 logger INFO: Saving checkpoint to training_artifacts/cars/model_tmp_lod2.pth
2025-03-22 11:50:26,173 logger INFO: 
[5/112] - ptime: 90.83, loss_d: 5.7863312, loss_g: 3.3090653, lae: 0.8485182, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 4839.627930",
2025-03-22 11:50:26,328 logger INFO: ################################################################################
2025-03-22 11:50:26,328 logger INFO: # Transition ended
2025-03-22 11:50:26,328 logger INFO: ################################################################################
2025-03-22 11:50:26,329 logger INFO: Batch size: 128, Batch size per GPU: 128, LOD: 2 - 16x16, blend: 1.000, dataset size: 11647
2025-03-22 11:51:56,396 logger INFO: Saving checkpoint to training_artifacts/cars/model_tmp_lod2.pth
2025-03-22 11:51:56,405 logger INFO: 
[6/112] - ptime: 90.06, loss_d: 3.9913890, loss_g: 2.5582743, lae: 0.6331831, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 4839.627930",
2025-03-22 11:51:56,482 logger INFO: ################################################################################
2025-03-22 11:51:56,483 logger INFO: # Switching LOD to 3
2025-03-22 11:51:56,483 logger INFO: # Starting transition
2025-03-22 11:51:56,483 logger INFO: ################################################################################
2025-03-22 11:51:56,483 logger INFO: Batch size: 64, Batch size per GPU: 64, LOD: 3 - 32x32, blend: 0.000, dataset size: 11647
2025-03-22 11:56:01,164 logger INFO: Saving checkpoint to training_artifacts/cars/model_tmp_lod3.pth
2025-03-22 11:56:01,317 logger INFO: 
[7/112] - ptime: 244.68, loss_d: 3.0709689, loss_g: 2.3380334, lae: 0.6860871, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 7564.132324",
2025-03-22 11:56:01,425 logger INFO: ################################################################################
2025-03-22 11:56:01,425 logger INFO: # Transition ended
2025-03-22 11:56:01,425 logger INFO: ################################################################################
2025-03-22 11:56:01,426 logger INFO: Batch size: 64, Batch size per GPU: 64, LOD: 3 - 32x32, blend: 1.000, dataset size: 11647
2025-03-22 12:00:02,940 logger INFO: Saving checkpoint to training_artifacts/cars/model_tmp_lod3.pth
2025-03-22 12:00:02,941 logger INFO: 
[8/112] - ptime: 240.59, loss_d: 1.9126856, loss_g: 1.7205539, lae: 0.6454039, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 7564.132324",
2025-03-22 12:00:02,963 logger INFO: ################################################################################
2025-03-22 12:00:02,963 logger INFO: # Switching LOD to 4
2025-03-22 12:00:02,963 logger INFO: # Starting transition
2025-03-22 12:00:02,964 logger INFO: ################################################################################
2025-03-22 12:00:02,964 logger INFO: Batch size: 32, Batch size per GPU: 32, LOD: 4 - 64x64, blend: 0.000, dataset size: 11647
2025-03-22 12:12:32,259 logger INFO: Saving checkpoint to training_artifacts/cars/model_tmp_lod4.pth
2025-03-22 12:12:32,396 logger INFO: 
[9/112] - ptime: 749.29, loss_d: 1.5205106, loss_g: 1.5289263, lae: 0.7393607, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 7564.132324",
2025-03-22 12:12:32,572 logger INFO: ################################################################################
2025-03-22 12:12:32,572 logger INFO: # Transition ended
2025-03-22 12:12:32,572 logger INFO: ################################################################################
2025-03-22 12:12:32,572 logger INFO: Batch size: 32, Batch size per GPU: 32, LOD: 4 - 64x64, blend: 1.000, dataset size: 11647
2025-03-22 12:21:08,491 logger INFO: Saving checkpoint to training_artifacts/cars/model_tmp_lod4.pth
2025-03-22 12:21:08,628 logger INFO: 
[10/112] - ptime: 515.92, loss_d: 1.2190827, loss_g: 1.5580462, lae: 0.7714209, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 7564.132324",
2025-03-22 12:21:08,853 logger INFO: ################################################################################
2025-03-22 12:21:08,872 logger INFO: # Switching LOD to 5
2025-03-22 12:21:08,909 logger INFO: # Starting transition
2025-03-22 12:21:08,909 logger INFO: ################################################################################
2025-03-22 12:21:08,909 logger INFO: Batch size: 16, Batch size per GPU: 16, LOD: 5 - 128x128, blend: 0.000, dataset size: 11647
2025-03-22 12:36:42,087 logger INFO: 
[11/112] - ptime: 933.15, loss_d: 1.1445601, loss_g: 1.6609753, lae: 0.8129808, blend: 0.951, lr: 0.001500000000,  0.001500000000, max mem: 7564.132324",
2025-03-22 12:39:19,098 logger INFO: Saving checkpoint to training_artifacts/cars/model_tmp_lod5.pth
2025-03-22 12:39:19,099 logger INFO: 
[11/112] - ptime: 1090.24, loss_d: 0.9972641, loss_g: 1.9398417, lae: 0.9636258, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 7564.132324",
2025-03-22 12:39:19,336 logger INFO: ################################################################################
2025-03-22 12:39:19,337 logger INFO: # Transition ended
2025-03-22 12:39:19,337 logger INFO: ################################################################################
2025-03-22 12:39:19,337 logger INFO: Batch size: 16, Batch size per GPU: 16, LOD: 5 - 128x128, blend: 1.000, dataset size: 11647
2025-03-22 12:54:50,377 logger INFO: 
[12/112] - ptime: 930.92, loss_d: 0.9994676, loss_g: 1.7650715, lae: 0.8090369, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 7564.132324",
2025-03-22 12:57:24,844 logger INFO: Saving checkpoint to training_artifacts/cars/model_tmp_lod5.pth
2025-03-22 12:57:24,845 logger INFO: 
[12/112] - ptime: 1084.01, loss_d: 0.9687172, loss_g: 1.6887603, lae: 0.7476984, blend: 1.000, lr: 0.001500000000,  0.001500000000, max mem: 7564.132324",
2025-03-22 12:57:24,877 logger INFO: ################################################################################
2025-03-22 12:57:24,877 logger INFO: # Switching LOD to 6
2025-03-22 12:57:24,877 logger INFO: # Starting transition
2025-03-22 12:57:24,878 logger INFO: ################################################################################
2025-03-22 12:57:24,878 logger INFO: Batch size: 16, Batch size per GPU: 16, LOD: 6 - 256x256, blend: 0.000, dataset size: 11647
